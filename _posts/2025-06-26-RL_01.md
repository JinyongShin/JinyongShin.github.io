---
title:  "강화 학습 - 기초 개념 정리"

categories:
  - AI 기술
tags:
  - 강화학습
  - AI
  - RL
  - ReinforcementLearning

author: JinyongShin
toc: true
toc_sticky: true
 
date: 2025-06-26 09:00:00 +0900
---

# 강화학습

강화학습은 인공지능 분야에서 가장 주목받는 기술 중 하나다. AlphaGo가 바둑에서 인간 챔피언을 이기고, 자율주행차가 도로를 달리며, 추천 시스템이 우리의 취향을 정확히 파악하는 뒤에는 모두 강화학습이 있다. 이 글에서는 개발자 관점에서 강화학습의 핵심 개념부터 실무 적용까지 체계적으로 살펴본다.

이 글은 KOOC에서 제공하는 카이스트 신하용 교수님의 [강화학습1](https://online.kaist.ac.kr/courses/66d16fba8004c3e1b006ae6a) 수업 내용을 기반으로 작성되었다. 아직 이해가 부족한 부분이 있으나, 일단 정리해두고 추후 업데이트해 나갈 예정이다.

## 강화학습이란 무엇인가

강화학습은 에이전트가 특정 환경 내에서 행동을 선택하고, 그 결과로 보상을 받으며, 이 보상을 최대화하도록 학습하는 과정이다. 핵심은 '시행착오'를 통해 학습한다는 점이다. 에이전트는 올바른 행동을 직접 알려주는 것이 아니라, 보상 신호를 통해 어떤 행동이 좋고 나쁜지 스스로 깨닫는다.

예를 들어, 컴퓨터가 비디오 게임을 플레이하며 점수를 최대화하는 방법을 스스로 찾아내거나, 로봇이 특정 작업을 수행하도록 움직임을 학습하는 것이 모두 강화학습의 범주에 속한다.

기존의 지도학습이나 비지도학습과의 차이점은 명확하다. 지도학습은 정답이 주어진 데이터로 학습하고, 비지도학습은 정답 없이 패턴을 찾는다. 반면 강화학습은 행동의 결과로 얻는 보상을 통해 최적의 행동 방식을 학습한다.

## 보상 가설: 강화학습의 철학

강화학습의 모든 것은 '보상 가설'에서 시작된다. "에이전트의 모든 목표는 누적 보상의 기대값을 최대화하는 것으로 설명될 수 있다"는 것이다. 강화학습의 모든 문제와 목표는 결국 이 '보상 최대화'로 귀결된다.

여기서 중요한 개념이 리턴(Return)이다. 리턴은 특정 시점 t부터 에피소드 종료 시점까지 얻는 모든 미래 보상의 합계를 의미한다.


$G_t = R_t + γR_{t+1} + γ²R_{t+2} + ... = Σ_{k=0}^{∞} γᵏR_{t+k}$




여기서 γ는 할인 인자(Discount Factor)로, 0과 1 사이의 값이다. 이는 미래 보상에 대한 현재의 중요도를 조절한다. γ가 0에 가까우면 에이전트는 즉각적인 보상에만 집중하는 '근시안적' 행동을 하고, γ가 1에 가까우면 장기적인 보상을 고려하는 '원시안적' 행동을 한다.

## 강화학습의 핵심 구성 요소

### 정책 (Policy, π)

정책은 에이전트의 '행동 방식'이다. 특정 상태에서 어떤 행동을 할지 결정하는 규칙을 의미한다.

- **결정론적 정책**: $A_t = π(S_t)$ (특정 상태에서 항상 같은 행동)
- **확률론적 정책**: $A_t ~ π(a|S_t)$ (특정 상태에서 각 행동을 선택할 확률)

강화학습 알고리즘은 경험을 통해 이 정책을 점진적으로 개선한다.

### 가치 함수 (Value Function)

가치 함수는 '얼마나 좋은 상태인가?' 또는 '얼마나 좋은 행동인가?'를 수치적으로 나타낸다. 미래에 얻을 것으로 예상되는 누적 보상의 기댓값이다.

- **상태 가치 함수** $V^π(s)$: 특정 정책 π를 따를 때, 상태 s에서 시작했을 때 얻을 것으로 예상되는 리턴의 기댓값
- **행동 가치 함수** $Q^π(s,a)$: 특정 정책 π를 따를 때, 상태 s에서 행동 a를 취한 후 얻을 것으로 예상되는 리턴의 기댓값

## Markov Decision Process (MDP)

강화학습 문제를 수학적으로 정형화하는 프레임워크가 MDP다. MDP는 5가지 요소로 구성된다.

- **S**: 상태 집합
- **A**: 행동 집합  
- **P**: 상태 전이 확률. $p(s', r | s, a)$는 상태 s에서 행동 a를 취했을 때, 다음 상태 s'와 보상 r이 나타날 확률
- **R**: 보상 함수. $R(s, a)$는 상태 s에서 행동 a를 취했을 때 기대되는 즉각적인 보상
- **γ**: 할인 인자

MDP 프레임워크의 유연성은 매우 크다. 시간 스텝이 실제 시간을 의미하지 않아도 되며, 행동과 상태도 다양한 수준으로 추상화될 수 있다. 로봇 팔 제어의 저수준 행동부터 점심 메뉴 결정의 고수준 행동까지 모두 MDP로 모델링할 수 있다.

## Bellman 방정식: 가치 함수의 핵심

Bellman 방정식은 가치 함수들 간의 재귀적인 관계를 설명한다.

### 예측 (Prediction)

Bellman 기대 방정식은 특정 정책 π를 따를 때, 현재 상태의 가치가 다음 상태들의 가치와 어떻게 연결되는지 나타낸다.

$V^π(s) = Σ_a π(a|s) Q^π(s,a)$
$Q^π(s,a) = E[R_{t+1} + γV^π(S_{t+1}) | S_t=s, A_t=a]$


직관적으로 말하면, 현재 상태의 가치는 현재 행동으로부터 얻을 즉각적인 보상과 이어지는 모든 가능한 다음 상태들의 할인된 가치의 합으로 표현된다.

### 제어 (Control)

Bellman 최적 방정식은 최적 정책을 따를 때의 최적 가치 함수를 정의한다.

$V*(s) = max_a Q*(s,a)$
$Q*(s,a) = E[R_{t+1} + γ max_{a'} Q*(S_{t+1},a') | S_t=s, A_t=a]$

최적 정책은 최적 가치 함수에 대해 항상 '탐욕적인' 행동을 취하는 정책이다. 단 한 스텝만 내다보고 가장 좋은 행동을 선택하더라도, 이미 V*에 모든 미래 보상 결과가 반영되어 있으므로 장기적으로도 최적이다.

## 동적 계획법 (Dynamic Programming)

MDP 모델을 완벽하게 알고 있을 때 최적 정책을 계산하는 알고리즘 집합이 동적 계획법이다.

### 정책 평가 (Policy Evaluation)

주어진 정책 π에 대한 상태 가치 함수 $V^π$ 를 계산한다. 반복 정책 평가는 Bellman 기대 방정식을 반복적으로 적용하여 $V^π$를 수렴시킨다.

### 정책 개선 (Policy Improvement)

현재 정책 $π$의 $V^π$ 를 기반으로, 각 상태에서 $V^π$ 에 대해 '탐욕적인' 새로운 정책 $π'$ 를 만든다. 정책 개선 정리에 따르면, 이 $π'$는 $π$보다 같거나 항상 더 좋다.

### 정책 반복과 가치 반복

- **정책 반복**: 정책 평가와 정책 개선을 반복하여 최적 정책에 수렴
- **가치 반복**: Bellman 최적 방정식을 직접 반복적으로 적용하여 $V^*$를 찾고, 그로부터 $π^*$를 도출

대표적인 예시로는 잭스 렌터카 문제가 있다. 자동차 대여량을 최적화하는 문제로, 동적 계획법의 실제 적용을 보여준다.

## 동적 계획법의 한계: 차원의 저주

동적 계획법은 'Full-width backup'을 사용한다. 각 업데이트마다 모든 가능한 다음 상태의 가치를 고려한다는 뜻이다. 하지만 상태 공간이 매우 크거나 연속적일 경우, 모든 상태에 대한 가치를 저장하고 모든 가능한 전이를 계산하는 것이 불가능에 가깝다.

상태가 이진 변수 n개로 구성되면 상태 공간은 $2^n$으로 기하급수적으로 증가한다. 이것이 바로 '차원의 저주'다.

해결책은 두 가지다:
- **샘플 기반 학습**: 실제 경험만을 사용하여 업데이트
- **함수 근사**: 대규모 상태 공간을 효율적으로 표현하고 가치를 추정

## 몬테카를로 방법

모델에 대한 사전 지식 없이 '경험'을 통해서만 학습하는 첫 번째 방법이다.

### 동작 방식

에피소드가 완전히 종료된 후, 해당 에피소드에서 관찰된 **실제 리턴**을 사용하여 가치 함수를 업데이트한다.

$V(S_t) ← V(S_t) + α(G_t - V(S_t))$

### 장단점

**장점:**
- 모델이 필요 없다 (Model-free)
- Bellman 방정식의 재귀적 의존성 없이 실제 리턴을 사용하므로, Bellman 방정식이 적용되지 않는 문제에도 적용 가능

**단점:**
- 에피소드가 끝날 때까지 기다려야 업데이트가 가능하므로, 긴 에피소드에는 비효율적
- 업데이트의 분산이 높을 수 있음
- Off-policy 학습 시 중요도 샘플링 필요

## 시간차 학습 (Temporal Difference)

몬테카를로와 동적 계획법의 아이디어를 결합한 방법이다. '경험으로부터 학습'하는 몬테카를로의 장점과 '부트스트래핑'하는 동적 계획법의 장점을 모두 가진다.

부트스트래핑이란 자신의 현재 가치 추정치를 사용하여 업데이트하는 방식이다. (몬테카를로는 실제 리턴 사용)

### TD(0) 예측

$V(S_t)$를 '한 스텝 앞'의 추정된 리턴으로 업데이트한다.

$V(S_t) ← V(S_t) + α(R_{t+1} + γV(S_{t+1}) - V(S_t))$

### Sarsa (On-policy TD control)

$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$의 '다섯 요소'를 사용하여 $Q(S_t, A_t)$를 업데이트한다.

$Q(S_t, A_t) ← Q(S_t, A_t) + α(R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$

$A_{t+1}$은 현재 $Q$를 따르는 엡실론-탐욕 정책에 따라 선택된다.

### Q-learning (Off-policy TD control)

행동을 선택하는 정책과 학습하려는 최적 정책이 다를 수 있다.

$Q(S_t, A_t) ← Q(S_t, A_t) + α(R_{t+1} + γ max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t))$

$S_t$에서 $A_t$를 선택한 후, $S_{t+1}$에서는 모든 가능한 행동 중 '최대 Q값'을 선택한다.

### Sarsa vs Q-learning

절벽 보행(Cliff Walking) 예시에서 차이가 명확하게 드러난다:
- **Q-learning**: 최적 정책의 Q값을 학습하지만, ε-탐욕 정책 때문에 가끔 절벽 아래로 떨어질 수 있음
- **Sarsa**: 실제로 따르는 정책을 고려하므로, 최적은 아니지만 '안전한' 우회 경로를 학습

### Expected Sarsa

$Q(S_t, A_t)$를 다음 상태에서 가능한 모든 행동의 Q값을 정책에 따라 평균낸 값으로 업데이트한다.

$Q(S_t, A_t) ← Q(S_t, A_t) + α(R_{t+1} + γE[Q(S_{t+1}, a') | S_{t+1}] - Q(S_t, A_t))$

Sarsa보다 계산 복잡성이 높지만, $A_{t+1}$의 무작위성에 의한 분산이 제거되어 일반적으로 성능이 더 좋다.

## n-Step Bootstrapping

몬테카를로는 전체 에피소드의 리턴을 사용하고, TD(0)는 한 스텝만 보고 업데이트한다. n-Step Bootstrapping은 이 둘 사이의 스펙트럼을 제공한다.

### n-스텝 리턴

처음 n 스텝까지는 실제 보상을 사용하고, n 스텝 이후의 상태에 대해서는 가치 함수 추정치를 사용하여 리턴을 계산한다.

$G_{t:t+n} = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n V(S_{t+n})$

몬테카를로와 TD(0) 중 어느 하나가 항상 최고는 아니며, 문제에 따라 최적의 n 값이 존재한다. 중간 정도의 n 값이 더 좋은 성능을 보이는 경우가 많다.

## Eligibility Traces (λ-Return)

n-Step Bootstrapping과 유사하게 몬테카를로(λ=1)와 TD(0)(λ=0) 사이를 부드럽게 연결하는 또 다른 방법이다.

### λ-Return

다양한 n-스텝 리턴에 'λ' 기반의 지수적으로 감소하는 가중치를 부여하여 합산한 값이다.

### Backward View (Eligibility Trace)

효율적인 온라인 업데이트를 위해 도입된 개념이다. z_t는 과거 상태-행동 쌍들이 얼마나 '최근에 방문되었고', '자주 방문되었는지'에 대한 흔적을 저장하는 벡터다.

z_t를 사용하여 현재의 TD 에러를 과거 상태-행동 쌍들에게 '신용'을 할당하여 가치 함수를 업데이트한다.

산악 자동차(Mountain Car) 예시에서, λ를 적절히 사용했을 때 n-스텝 Sarsa보다 효율적인 학습 성능을 보였다.

## 함수 근사 (Function Approximation)

### 필요성

동적 계획법, 몬테카를로, 시간차 방법들은 상태-행동 쌍의 가치를 테이블 형태로 저장한다. 하지만 상태 공간이 너무 크거나 연속적인 경우, 테이블 방식은 불가능하다.

체스 보드의 상태 수나 로봇 팔 관절 각도와 같은 연속적인 상태를 생각해보라. 이런 경우 차원의 저주가 발생한다.

### 아이디어

가치 함수를 테이블이 아닌, 몇 개의 매개변수로 표현되는 '함수'로 근사한다.

$V(s) ≈ v̂(s, w)$
$Q(s,a) ≈ q̂(s,a, w)$

이 함수는 선형 함수, 인공 신경망, 결정 트리 등이 될 수 있다.

### 선형 함수 근사

상태 또는 상태-행동 쌍의 특징 벡터 $x(s)$ 또는 $x(s,a)$와 가중치 벡터 w의 내적으로 가치 함수를 표현한다.

$v̂(s,w) = w^T x(s)$

특징 선택이 중요하며, 타일 코딩, 푸리에 기저, 방사형 기저 함수 등이 사용된다.

### 세미-경사 방법

부트스트랩을 사용하는 TD 방식에서는 업데이트 목표 값이 현재의 매개변수 $w$에 의존하기 때문에, 완전한 '경사 하강'은 아니다. 이를 '세미-경사'라고 부른다.

## 치명적인 삼중주 (Deadly Triad)

함수 근사의 가장 큰 도전 과제다. 다음 세 가지 요소가 동시에 사용될 때 강화학습 알고리즘이 불안정해지거나 발산할 수 있다:

1. **함수 근사**: 대규모 상태 공간을 다루기 위함
2. **부트스트래핑**: 자신의 가치 추정치를 사용하여 업데이트하는 방식 (TD 방식)
3. **Off-policy Learning**: 행동 정책과 학습하려는 목표 정책이 다른 경우

### Baird의 반례

이 '치명적인 삼중주'가 얼마나 심각한 문제인지 보여주는 단순한 예시다. 특정 조건에서 선형 함수 근사를 사용하는 Off-policy TD(0)가 불안정하게 발산함을 보여준다.

### 해결책

이 문제를 해결하기 위한 연구가 활발히 진행 중이다:

- **Gradient-TD 방법**: Bellman Error를 직접적으로 최소화하는 방향으로 학습하는 알고리즘
- **Emphatic-TD 방법**: 특정 상태에 대한 '강조' 값을 두어, 해당 상태에서 발생하는 업데이트의 중요도를 조절
- **안전한 함수 근사 방법**: '평균자'라고 불리는 방법들은 안정성을 보장하지만, 표현력이 제한적
- **정책 경사 방법**: 가치 함수를 직접 근사하기보다 정책 자체를 직접적으로 최적화

## 강화학습의 실제 응용 사례

강화학습은 이미 다양한 분야에서 뛰어난 성과를 보여주고 있다.

### 게임 AI

- **TD-Gammon** (1992): 제로 시작하여 백개먼 세계 최고 수준의 플레이 달성
- **Samuel's Checkers Player** (1959): 자가 학습을 통해 세계 챔피언 수준의 체커 플레이 달성
- **Watson의 Daily-Double 배팅**: 퀴즈쇼 Jeopardy!에서 인간 챔피언을 능가하는 배팅 전략 학습
- **AlphaGo & AlphaGo Zero**: 딥러닝과 몬테카를로 트리 탐색을 결합하여 바둑에서 인간 챔피언을 이김. AlphaGo Zero는 인간 기보 없이 순수 자가 학습으로 더 높은 성능 달성

### 다양한 분야

- **로봇 제어**: 자율 주행, 로봇 팔 조작
- **추천 시스템**: 개인화된 웹 서비스, 광고 추천
- **자원 관리**: 컴퓨터 메모리 제어, 데이터 센터 관리, 통신 네트워크 큐 관리
- **재무 관리, 에너지 관리, 제조**: 다양한 최적화 문제

## 자주 묻는 질문들

### 강화학습이 왜 요즘 더 중요해졌나요?

강화학습은 오랫동안 연구되어 왔지만, 최근 몇 년간 그 중요성이 크게 부각되었다. 주된 이유는 다음과 같다:

1. **계산 능력의 발전**: GPU와 같은 하드웨어의 발전으로 복잡한 계산을 더 빠르게 수행할 수 있게 되었다
2. **데이터의 증가**: 방대한 데이터를 통해 에이전트가 더 많은 경험을 쌓고 학습할 수 있는 환경이 조성되었다
3. **딥러닝과의 결합**: 인공 신경망을 함수 근사자로 활용하는 '심층 강화학습'이 등장하면서, 대규모의 복잡한 상태 공간에서도 효과적으로 가치 함수나 정책을 표현하고 학습할 수 있게 되었다. AlphaGo와 같은 성공 사례들은 이러한 결합의 강력함을 입증했다.

### 강화학습을 개발자가 어떻게 활용할 수 있을까요?

강화학습은 다양한 자율 시스템 및 최적화 문제에 적용될 수 있다:

- **게임 AI 개발**: 복잡한 게임 환경에서 인간 수준을 넘어서는 AI 플레이어 개발
- **로봇 제어**: 로봇 팔 조작, 보행 로봇의 움직임 제어, 자율 주행 자동차 등
- **추천 시스템**: 사용자 행동에 따른 개인화된 콘텐츠나 상품 추천 최적화
- **자원 관리/스케줄링**: 클라우드 컴퓨팅 자원 할당, 전력망 최적화, 제조 공정 스케줄링 등 복잡한 시스템의 효율성 극대화
- **금융 거래**: 시장 데이터를 기반으로 최적의 투자 전략 수립
- **헬스케어**: 개인 맞춤형 치료 계획 수립

### Monte Carlo와 TD의 가장 큰 차이점은 무엇인가요?

두 방법 모두 모델을 알지 못하는 상황에서 경험을 통해 학습하지만, 업데이트 방식에서 큰 차이를 보인다.

**Monte Carlo (MC):**
- **업데이트 시점**: 에피소드가 완전히 끝난 후에야 업데이트
- **업데이트 대상**: 실제 관찰된 전체 리턴을 목표 값으로 사용
- **장점**: 편향되지 않은 추정치를 제공하며, MDP 모델이 불완전하거나 없는 경우에도 적용 가능
- **단점**: 긴 에피소드에서는 업데이트가 지연되고, 업데이트의 분산이 높음

**Temporal Difference (TD):**
- **업데이트 시점**: 각 시간 스텝마다 '온라인'으로 업데이트할 수 있음
- **업데이트 대상**: 미래에 대한 자신의 '현재 추정치'를 사용하여 리턴을 근사하고 업데이트 (부트스트래핑)
- **장점**: MC보다 업데이트가 빠르고 효율적이며, 일반적으로 분산이 낮음. 온라인 학습에 적합
- **단점**: 부트스트랩으로 인해 '편향'될 수 있으며, 초기 가치 추정의 오류가 전파될 수 있음

### Epsilon-greedy 정책은 무엇이고 왜 사용하나요?

Epsilon-greedy 정책은 강화학습에서 **탐험(Exploration)**과 **활용(Exploitation)**의 균형을 맞추기 위해 사용되는 전략이다.

- **활용**: 현재까지 학습된 지식을 바탕으로 가장 좋다고 생각되는 행동을 선택하는 것
- **탐험**: 아직 시도해보지 않았거나 불확실한 행동을 시도해보는 것

**동작 방식:**
- 대부분의 시간(확률 1-ε): 현재 Q값 기준으로 가장 높은 행동을 선택(활용)
- 작은 확률(ε): 가능한 행동 중 하나를 무작위로 선택(탐험)

**사용 이유:**
- 만약 항상 활용만 한다면, 에이전트는 초기에 발견한 좋은 행동에만 머물러서 '지역 최적해'에 갇힐 수 있다
- 탐험을 통해 에이전트는 새로운 상태-행동 공간을 탐색하고, 잠재적으로 더 큰 보상을 가져다줄 '전역 최적해'를 찾을 기회를 얻는다
- 학습 초기에는 ε을 크게 설정하여 탐험을 장려하고, 학습이 진행될수록 ε을 점차 줄여 활용을 늘리는 방식을 사용하기도 한다

### "Deadly Triad" 문제를 어떻게 해결하나요?

"Deadly Triad"는 함수 근사, 부트스트랩, Off-policy 학습이 결합될 때 발생하는 강화학습 알고리즘의 불안정성 문제다. 이를 해결하기 위한 주요 접근 방식들은 다음과 같다:

**1. Gradient-TD (GTD) Methods:**
- Bellman Error를 직접적으로 최소화하는 방향으로 학습하는 알고리즘
- TD 학습의 목표 값이 매개변수 w에 의존하여 발생하는 '세미-경사' 문제를 보완하여, 수렴이 보장되는 알고리즘을 만듦 (예: GTD2)
- 계산 복잡성이 증가할 수 있음

**2. Emphatic-TD Methods:**
- 특정 상태에 대한 '강조' 값을 두어, 해당 상태에서 발생하는 업데이트의 중요도를 조절하는 방식
- 이 강조 값은 에이전트가 어떤 상태를 더 중요하게 학습해야 하는지를 나타내며, Off-policy 데이터 사용 시 안정성을 개선하는 데 도움을 줌

**3. 안전한 함수 근사 방법 사용:**
- 모든 함수 근사 방법이 '치명적인 삼중주'에 취약한 것은 아님
- '평균자'라고 불리는 방법들(최근접 이웃 방법, 지역 가중 회귀)은 안정성을 보장하지만, 인공 신경망이나 타일 코딩처럼 널리 사용되는 방법들은 그렇지 않음
- 하지만 이러한 방법들은 복잡한 문제에 대한 표현력이 제한적일 수 있음

**4. 정책 경사(Policy Gradient) 방법:**
- 가치 함수를 직접 근사하기보다 정책 자체를 직접적으로 최적화하는 방법
- 일반적으로 FA와 Off-policy 학습을 같이 사용하더라도 부트스트랩을 통한 가치 함수 업데이트가 주요 문제가 아니므로 더 안정적일 수 있음

## 결론 및 다음 단계

강화학습은 보상 최대화를 목표로 시행착오를 통해 학습하는 강력한 패러다임이다. MDP, Bellman 방정식, 동적 계획법, 몬테카를로, 시간차 학습은 강화학습의 핵심 이론적 기반을 형성한다.

함수 근사는 대규모 및 연속 공간 문제 해결에 필수적이지만, 함수 근사 + 부트스트래핑 + Off-policy 학습의 '치명적인 삼중주'는 불안정성을 야기할 수 있다. 이러한 문제들을 해결하기 위한 연구가 활발히 진행 중이며, 실제로도 다양한 해결책들이 제시되고 있다.

강화학습은 게임, 로봇, 추천 시스템 등 다양한 분야에서 이미 뛰어난 성과를 보여주고 있다. 특히 딥러닝과의 결합을 통해 그 가능성은 무한히 확장되고 있다.

본 글에서는 강화학습의 '가치 기반' 방법에 초점을 맞췄다. 개발자들은 이제 심층 강화학습(Deep Reinforcement Learning, DQN 등), 정책 경사(Policy Gradient) 방법, 액터-크리틱(Actor-Critic) 알고리즘 등 더 진보된 주제로 나아갈 수 있다.

강화학습은 단순히 이론적 관심사가 아니라, 실제 문제를 해결할 수 있는 강력한 도구다. 개발자로서 이러한 기초를 탄탄히 다진 후, 실제 프로젝트에 적용해보는 것이 다음 단계가 될 것이다.